{
  "cells": [
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T19:02:36.256259Z",
          "start_time": "2024-12-18T19:02:35.838278Z"
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import helper\n",
        "from youtube.config import youtube_watch_history_file, video_details_download_folder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "key = helper.get_google_api_key()\n",
        "\n",
        "df = pd.read_json(youtube_watch_history_file)\n",
        "\n",
        "# extract video_id from titleUrl\n",
        "df['video_id'] = df['titleUrl'].str.extract(r'v=(.*)')\n",
        "\n",
        "# keep rows where 'details' column is NaN which means not Ads\n",
        "df = df[df['details'].isna()].reset_index()\n",
        "\n",
        "# create a list of video_ids from the existing 'video_id' column\n",
        "video_ids = df['video_id'].tolist()\n",
        "print(f\"total videos: {len(video_ids)}\")\n",
        "\n",
        "# warn and stop if there are more than 10000 videos\n",
        "# remove this if you know what you are doing and have an API key with higher quota\n",
        "if len(video_ids) > 10000:\n",
        "    print(\"WARNING: \"\n",
        "          \"You have more than 10,000 videos in your history. Please use the splitter.py script to split the watch history file into smaller files before proceeding. The Google API has a limit of 10,000 requests per day. After splitting the file, you can run this script for each file separately.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# download all video details in parallel\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import requests\n",
        "\n",
        "print(\"start downloading ... please wait ...\")\n",
        "start = time.perf_counter()\n",
        "\n",
        "\n",
        "def download_image(vid):\n",
        "    response = requests.get(\n",
        "        \"https://content.googleapis.com/youtube/v3/videos?id={}&part=contentDetails,snippet&key={}\".format(vid, key))\n",
        "    data = response.json()\n",
        "    data = json.dumps(data, indent=4)\n",
        "    with open(os.path.join(video_details_download_folder, f'{vid}.json'), 'w') as f:\n",
        "        f.write(str(data))\n",
        "\n",
        "\n",
        "# todo check cpu vs. cores etc.\n",
        "max_workers = os.cpu_count()\n",
        "print(f\"max_workers: {max_workers}\")\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    executor.map(download_image, video_ids)\n",
        "\n",
        "print(f\"total time: {time.perf_counter() - start}\")\n",
        "print(\"done downloading ... please wait ...\")\n"
      ],
      "id": "4a6feb1ab54ca4c8",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "",
      "id": "9b224c2bd628bcdd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 5,
  "nbformat_minor": 9
}
