{
  "cells": [
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:22.097646Z",
          "start_time": "2024-12-13T12:18:21.476791Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "from youtube.config import ignore_videos_longer_than_seconds, \\\n",
        "    assume_all_videos_were_watched_to_factor, ignore_videos_from_channel_regex, youtube_watch_history_file, \\\n",
        "    video_details_download_folder, output_dir_unique, year_to_analyze\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def parse_to_seconds(duration_iso):\n",
        "    try:\n",
        "        if 'H' in duration_iso:\n",
        "            hours = int(duration_iso.split('H')[0].split('T')[1]) if duration_iso.split('H')[0].split('T')[1] else 0\n",
        "            minutes = int(duration_iso.split('H')[1].split('M')[0]) if duration_iso.split('H')[1].split('M')[0] else 0\n",
        "            seconds = int(duration_iso.split('M')[1].split('S')[0]) if duration_iso.split('M')[1].split('S')[0] else 0\n",
        "            return hours * 3600 + minutes * 60 + seconds\n",
        "        elif 'M' in duration_iso:\n",
        "            minutes = int(duration_iso.split('M')[0].split('T')[1]) if duration_iso.split('M')[0].split('T')[1] else 0\n",
        "            seconds = int(duration_iso.split('M')[1].split('S')[0]) if duration_iso.split('M')[1].split('S')[0] else 0\n",
        "            return minutes * 60 + seconds\n",
        "        else:\n",
        "            return int(duration_iso.split('S')[0].split('T')[1]) if duration_iso.split('S')[0].split('T')[1] else 0\n",
        "    except (IndexError, ValueError):\n",
        "        return -1\n",
        "\n",
        "df = pd.read_json(youtube_watch_history_file)\n",
        "\n",
        "# extract video_id from titleUrl\n",
        "df['video_id'] = df['titleUrl'].str.extract(r'v=(.*)')\n",
        "# keep rows where 'details' column is NaN which means not Ads\n",
        "df = df[df['details'].isna()].reset_index()\n",
        "\n",
        "# load video details from raw file\n",
        "def load_all_video_details(directory):\n",
        "    tmp = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.json'):\n",
        "            video_id = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            with open(file_path, 'r') as file:\n",
        "                video_details = json.load(file)\n",
        "                tmp[video_id] = video_details\n",
        "    return tmp\n",
        "\n",
        "video_details_map = load_all_video_details(video_details_download_folder)\n",
        "\n",
        "def get_duration_seconds(video_id):\n",
        "    if video_id in video_details_map:\n",
        "        items = video_details_map[video_id].get('items', [])\n",
        "        if items:\n",
        "            return parse_to_seconds(items[0]['contentDetails']['duration'])\n",
        "    return None\n",
        "\n",
        "def get_channel_url(video_id):\n",
        "    if video_id in video_details_map:\n",
        "        items = video_details_map[video_id].get('items', [])\n",
        "        if items:\n",
        "            channel_id = items[0]['snippet']['channelId']\n",
        "            return f'https://www.youtube.com/channel/{channel_id}'\n",
        "    return None\n",
        "\n",
        "# Ensure the 'time' column is of datetime type\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "# Convert the 'time' column to Berlin timezone\n",
        "berlin_tz = pytz.timezone('Europe/Berlin')\n",
        "df['time'] = df['time'].dt.tz_convert(berlin_tz)\n",
        "# Add weekday and minutes played columns\n",
        "df['weekday'] = df['time'].dt.day_name()\n",
        "df['year'] = df['time'].dt.year\n",
        "\n",
        "df['duration_seconds'] = round(df['video_id'].map(get_duration_seconds))\n",
        "df['duration_seconds'] = assume_all_videos_were_watched_to_factor * df['duration_seconds']\n",
        "\n",
        "df['duration_minutes'] = round(df['duration_seconds'] / 60)\n",
        "df['duration_hours'] = round(df['duration_seconds'] / 3600)\n",
        "\n",
        "df['channel_url'] = df['video_id'].map(get_channel_url)\n",
        "df['video_url'] = df['video_id'].map(lambda x: f'https://www.youtube.com/watch?v={x}')\n",
        "\n",
        "# Add hour of the day\n",
        "df['hour'] = df['time'].dt.hour\n",
        "\n",
        "# Remove videos that are longer than 2 hours (7200 seconds)\n",
        "df = df[df['duration_seconds'] <= ignore_videos_longer_than_seconds]\n",
        "\n",
        "# remove videos if year_to_analyze is set\n",
        "if year_to_analyze != 'all':\n",
        "    df = df[df['year'] == int(year_to_analyze)]\n",
        "\n",
        "# add channel title\n",
        "df['channel_title'] = df['video_id'].map(lambda x: video_details_map[x]['items'][0]['snippet']['channelTitle'])\n",
        "\n",
        "# remove all videos that match ignore_videos_from_channel_regex\n",
        "for regex in ignore_videos_from_channel_regex:\n",
        "    df = df[~df['channel_title'].str.contains(regex, case=False)]\n",
        "\n",
        "# remove all videos that have duration_seconds NaN which means the video is not available anymore\n",
        "df = df[~df['duration_seconds'].isna()]\n",
        "# remove header column\n",
        "df = df.drop(columns=['header', 'titleUrl', 'subtitles', 'products', 'activityControls', 'details', 'index', 'description'])\n"
      ],
      "id": "4a6feb1ab54ca4c8",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:22.571190Z",
          "start_time": "2024-12-13T12:18:22.104639Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the total watched hours per year\n",
        "df['watched_hours'] = df['duration_seconds'] / 3600\n",
        "total_watched_hours_per_year = df.groupby('year')['watched_hours'].sum().reset_index()\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(total_watched_hours_per_year['year'].astype(str), total_watched_hours_per_year['watched_hours'], color='skyblue')\n",
        "plt.title('Total Watched Hours Per Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Watched Hours')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir_unique, 'total_watched_hours_per_year.png'))\n",
        "plt.show()"
      ],
      "id": "3d5f1ca43bbd65d6",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:23.016090Z",
          "start_time": "2024-12-13T12:18:22.650164Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure the 'time' column is of datetime type\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "\n",
        "# Extract the year and month from the 'time' column\n",
        "df['year'] = df['time'].dt.year\n",
        "df['month'] = df['time'].dt.month\n",
        "\n",
        "# Calculate the total watched hours per month for each year\n",
        "df['watched_hours'] = df['duration_seconds'] / 3600\n",
        "total_watched_hours_per_month = df.groupby(['year', 'month'])['watched_hours'].sum().reset_index()\n",
        "\n",
        "# Pivot the data to have years as columns and months as rows\n",
        "pivot_df = total_watched_hours_per_month.pivot(index='month', columns='year', values='watched_hours')\n",
        "\n",
        "# Plot the results\n",
        "pivot_df.plot(kind='line', marker='o', figsize=(12, 6))\n",
        "plt.title('Total Watched Hours Per Month for Each Year')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Watched Hours')\n",
        "plt.xticks(range(1, 13))\n",
        "plt.grid(True)\n",
        "plt.legend(title='Year')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir_unique, 'total_watched_hours_per_month.png'))\n",
        "plt.show()"
      ],
      "id": "c5bedfeda995a596",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:23.361606Z",
          "start_time": "2024-12-13T12:18:23.030153Z"
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group by channel_title and sum the duration_seconds\n",
        "channel_grouped = df.groupby('channel_title')['duration_seconds'].sum().reset_index()\n",
        "\n",
        "# Convert duration_seconds to hours\n",
        "channel_grouped['duration_hours'] = channel_grouped['duration_seconds'] / 3600\n",
        "\n",
        "# Sort the grouped data by duration_hours in descending order\n",
        "top_channels = channel_grouped.sort_values(by='duration_hours', ascending=False).head(30)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(top_channels['channel_title'], top_channels['duration_hours'], color='skyblue')\n",
        "plt.xlabel('Hours Streamed')\n",
        "plt.ylabel('Channel Title')\n",
        "plt.title('Top Channels by Hours Stream Time')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the highest value on top\n",
        "plt.savefig(os.path.join(output_dir_unique, 'top_channels_by_hours_streamed.png'), bbox_inches='tight')\n",
        "plt.show()"
      ],
      "id": "ad062db0cfbb8620",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:23.933868Z",
          "start_time": "2024-12-13T12:18:23.377064Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ensure the 'time' column is of datetime type\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaT values in the 'time' column\n",
        "df = df.dropna(subset=['time'])\n",
        "\n",
        "# Group by week and count the number of videos streamed\n",
        "df['week'] = df['time'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "videos_per_week = df.groupby('week').size().reset_index(name='video_count')\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(videos_per_week['week'], videos_per_week['video_count'], color='skyblue')\n",
        "plt.xlabel('Week')\n",
        "plt.ylabel('Number of Videos Streamed')\n",
        "plt.title('Number of Videos Streamed per Week')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 150)  # Limit y-axis to 150 videos\n",
        "plt.grid(True)\n",
        "\n",
        "# Add trendline\n",
        "z = np.polyfit(range(len(videos_per_week)), videos_per_week['video_count'], 1)\n",
        "p = np.poly1d(z)\n",
        "plt.plot(videos_per_week['week'], p(range(len(videos_per_week))), color='red', linestyle='--')\n",
        "plt.savefig(os.path.join(output_dir_unique, 'videos_streamed_per_week.png'), bbox_inches='tight')\n",
        "plt.show()"
      ],
      "id": "5aa4c5dbc28202c1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "",
      "id": "9b224c2bd628bcdd"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:24.278316Z",
          "start_time": "2024-12-13T12:18:23.950601Z"
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Calculate hours played\n",
        "df['duration_hours'] = df['duration_seconds'] / 3600\n",
        "\n",
        "# Group by day and calculate the total hours played for each day\n",
        "df['date'] = df['time'].dt.date\n",
        "daily_hours = df.groupby('date')['duration_hours'].sum().reset_index()\n",
        "\n",
        "# Calculate the 95th percentile value\n",
        "percentile_95 = daily_hours['duration_hours'].quantile(0.95)\n",
        "\n",
        "# Filter the DataFrame to only keep rows within the 95th percentile\n",
        "filtered_daily_hours = daily_hours[daily_hours['duration_hours'] <= percentile_95]\n",
        "\n",
        "# Calculate the average hours played per day for the filtered data\n",
        "average_daily_hours = filtered_daily_hours['duration_hours'].mean()\n",
        "\n",
        "# Define custom colormap\n",
        "colors = ['#d73027', '#fc8d59', '#fee08b', '#d9ef8b', '#91cf60', '#1a9850']\n",
        "cmap = mcolors.ListedColormap(colors)\n",
        "bounds = np.arange(0, len(colors) + 1, 1)\n",
        "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "sc = plt.scatter(filtered_daily_hours['date'], filtered_daily_hours['duration_hours'], c=filtered_daily_hours['duration_hours'], cmap=cmap, norm=norm, edgecolors='black')\n",
        "plt.colorbar(sc, ticks=bounds)\n",
        "plt.axhline(y=average_daily_hours, color='red', linestyle='--', label=f'Average: {average_daily_hours:.2f} hours/day')\n",
        "\n",
        "# Add trendline\n",
        "z = np.polyfit(range(len(filtered_daily_hours)), filtered_daily_hours['duration_hours'], 1)\n",
        "p = np.poly1d(z)\n",
        "plt.plot(filtered_daily_hours['date'], p(range(len(filtered_daily_hours))), color='blue', linestyle='--', label='Trendline')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Hours Played')\n",
        "plt.title('Average Hours Played per Day (Filtered to 95th Percentile)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(output_dir_unique, 'average_hours_played_per_day.png'), bbox_inches='tight')\n",
        "plt.show()"
      ],
      "id": "493935fd44dbd7bd",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:24.380537Z",
          "start_time": "2024-12-13T12:18:24.296874Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure the 'time' column is of datetime type\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "\n",
        "# Extract the year from the 'time' column\n",
        "df['year'] = df['time'].dt.year\n",
        "\n",
        "# Group by year and channel_title, and count the number of videos\n",
        "year_channel_video_count = df.groupby(['year', 'channel_title']).size().reset_index(name='video_count')\n",
        "\n",
        "# Sort the grouped data by year and video_count in descending order\n",
        "sorted_year_channel_video_count = year_channel_video_count.sort_values(by=['year', 'video_count'], ascending=[False, False])\n",
        "\n",
        "# Keep only the top ten channels per year\n",
        "top_ten_per_year = sorted_year_channel_video_count.groupby('year').head(10).reset_index(drop=True)\n",
        "\n",
        "# Add the channel_url to the result\n",
        "top_ten_per_year['channel_url'] = top_ten_per_year['channel_title'].map(lambda x: df[df['channel_title'] == x]['channel_url'].values[0])\n",
        "\n",
        "top_ten_per_year.to_csv(os.path.join(output_dir_unique, 'top_ten_channels_per_year.csv'), index=False)\n",
        "\n",
        "# Display the result\n",
        "top_ten_per_year"
      ],
      "id": "487d75f21f877b1e",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:24.459769Z",
          "start_time": "2024-12-13T12:18:24.426955Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure the 'time' column is of datetime type\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "\n",
        "# Extract the year from the 'time' column\n",
        "df['year'] = df['time'].dt.year\n",
        "\n",
        "# Sort the DataFrame by 'year' (descending) and 'channel_title'\n",
        "sorted_videos = df.sort_values(by=['year', 'channel_title'], ascending=[False, True])\n",
        "\n",
        "# Add the video_url column\n",
        "sorted_videos['video_url'] = sorted_videos['video_id'].map(lambda x: f'https://www.youtube.com/watch?v={x}')\n",
        "\n",
        "# Display the result\n",
        "sorted_videos_list = sorted_videos[['year', 'channel_title', 'title', 'video_url']].reset_index(drop=True)\n",
        "sorted_videos_list.to_csv(os.path.join(output_dir_unique, 'all_videos_by_year_channel.csv'), index=False)\n",
        "sorted_videos_list"
      ],
      "id": "c857b0c53d8a64ec",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-13T12:18:24.587943Z",
          "start_time": "2024-12-13T12:18:24.561673Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure the 'time' column is of datetime type\n",
        "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
        "\n",
        "# Group by video_id and count the occurrences\n",
        "video_play_counts = df.groupby('video_id').size().reset_index(name='play_count')\n",
        "\n",
        "# Filter to include only videos played more than once\n",
        "multiple_plays = video_play_counts[video_play_counts['play_count'] > 1]\n",
        "\n",
        "# Merge with the original DataFrame to get additional video details\n",
        "multiple_plays_details = multiple_plays.merge(df, on='video_id', how='left')\n",
        "\n",
        "# Sort by play_count in descending order\n",
        "multiple_plays_details = multiple_plays_details.sort_values(by='play_count', ascending=False)\n",
        "\n",
        "# Display the result\n",
        "multiple_plays_list = multiple_plays_details[['year', 'title', 'channel_title', 'play_count', 'video_url']].reset_index(drop=True)\n",
        "multiple_plays_list.to_csv(os.path.join(output_dir_unique, 'videos_played_multiple_times.csv'), index=False)\n",
        "\n",
        "multiple_plays_list"
      ],
      "id": "e402bfb1aeea8d42",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 5,
  "nbformat_minor": 9
}
